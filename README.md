# AI_Application_mid-term


University Honor Code:

"By signing this pledge, I promise to adhere to exam requirements and maintain the highest level of ethical principles during the exam period."


Student ID: 12194900
Name: Asatullaev Abdullokh


https://github.com/Abdullokh3/AI_Application_mid-term/blob/main/week_5_before2.ipynb
Actually this is the fourth lab session. In this lab session we learned how to get a CSV file from the given GitHub link. We also learned some essential codes that we use in order to display the CPU information as well as installing GPU. In general with the help of going through this lab, we got to know how to obtain data from various sources for example Kaggle, Google Drive or GitHub and so on. For instance we attempted to obtain mnist csv file from the kaggle. So as a conclusion these are the basic and different functions that we learned to perform in this lab:
• cloning the code from the GitHub
• how to display the file names as well as their content
• how to import files
• how to load and unzip files
• have to display the CVS file using the sklearn as well as installing packages

https://github.com/Abdullokh3/AI_Application_mid-term/blob/main/Week_5.ipynb
This is this 5th week lab work. In this lab session we got a massive knowledge regarding basic mathematical operations for example we learned about adding, subtracting, multiplying and other things with the help of tensorflow. At the same time we learnt basically what is the tensorflow and computational graph, constants, placeholders and variables in the tensorflow as well as using some commands to find tensorboard visualization for example histogram and scalar modes. Mainly tensorflow is a library providing various kinds of operations and functionality for deep learning models. It's made of two terms: tanser means representation as a multidimensional array of data; flow means series of operations being performed on the tensor.

https://github.com/Abdullokh3/AI_Application_mid-term/blob/main/week_6_Lab1_first_half.ipynb
https://github.com/Abdullokh3/AI_Application_mid-term/blob/main/week_6_Lab1_2nd_half.ipynb
This is 6th week lab first session. In this lab we learnt how to get data set plots as a line graph and we tried to add weight to the model as well as layer adding. First we started by installing the right version of tensorflow and then we imported the libraries, defined parameters, creating the data set of linear regression as well as graph plotting, model defining which we want to predict, define its optimizer implement cost function and initialize variables so that we execute the graph by the help of tensorflow session. Then we attempted to denoise an image from dataset by using some filters. 

https://github.com/Abdullokh3/AI_Application_mid-term/blob/main/week_6_2nd_Lab.ipynb
This is the second session of lab week 6. in this session we imported some data set of fashion mnist from collab. then we added specific layers for hidden as well as input, output. Batch, size of image and epochs were given then we had to add weight to all of them so that we created a computational graph. We made it possible using tensorflow API, providing five layers. Finally we could manage to plot original images.

https://github.com/Abdullokh3/AI_Application_mid-term/blob/main/week_7_Lab_1.ipynb
It is week 7 lab Session 1. We imported CIFAR10 data set using the pytorch API. we had to get the data from the dataset so we loaded it into a single batch then matplotlib was used as well as data loader so that we could draw an image plot.


https://github.com/Abdullokh3/AI_Application_mid-term/blob/main/week_7_Lab_2.ipynb
This is a link for week 7 lab session 2. In this lab session we imported all the necessary libraries and we also imported the data. we gave batch size and and epoch size whereas, the data was downloaded from the torch Library. We also defined what is the test and what is the train data then we added convolutional layers, the size was 3x3 which means this is a multiplication with 3 by 3 size matrix. ReLu was the activation. In order to train the model we have to give the necessary weights. We defined the loss function, optimizer and finally trained the model.
